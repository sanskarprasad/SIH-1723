{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pathlib import Path\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = Path('/home/nitin/Downloads/SIH-1723/')\n",
    "results_path = base_dir / 'results'\n",
    "model_path = base_dir / 'ml_models' / 'models'\n",
    "\n",
    "# Load data\n",
    "X_train = pd.read_csv(base_dir / 'data/processed/forward/X_train_Elongation.csv')\n",
    "X_test = pd.read_csv(base_dir / 'data/processed/forward/X_test_Elongation.csv')\n",
    "y_train = pd.read_csv(base_dir / 'data/processed/forward/y_train_Elongation.csv').squeeze()\n",
    "y_test = pd.read_csv(base_dir / 'data/processed/forward/y_test_Elongation.csv').squeeze()\n",
    "A_train = pd.read_csv(base_dir / 'data/processed/forward/X_train_UTS.csv')\n",
    "A_test = pd.read_csv(base_dir / 'data/processed/forward/X_test_UTS.csv')\n",
    "b_train = pd.read_csv(base_dir / 'data/processed/forward/y_train_UTS.csv').squeeze()\n",
    "b_test = pd.read_csv(base_dir / 'data/processed/forward/y_test_UTS.csv').squeeze()\n",
    "R_train = pd.read_csv(base_dir / 'data/processed/forward/X_train_Conductivity.csv')\n",
    "R_test = pd.read_csv(base_dir / 'data/processed/forward/X_test_Conductivity.csv')\n",
    "s_train = pd.read_csv(base_dir / 'data/processed/forward/y_train_Conductivity.csv').squeeze()\n",
    "s_test = pd.read_csv(base_dir / 'data/processed/forward/y_test_Conductivity.csv').squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, A_train, A_test, R_train, R_test, y_train, y_test, b_train, b_test, s_train, s_test):\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler_y = MinMaxScaler()\n",
    "    target_scaler_b = MinMaxScaler()\n",
    "    target_scaler_s = MinMaxScaler()\n",
    "\n",
    "    # Scale the features\n",
    "    X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = feature_scaler.transform(X_test)\n",
    "    A_train_scaled = feature_scaler.fit_transform(A_train)\n",
    "    A_test_scaled = feature_scaler.transform(A_test)\n",
    "    R_train_scaled = feature_scaler.fit_transform(R_train)\n",
    "    R_test_scaled = feature_scaler.transform(R_test)\n",
    "\n",
    "    # Scale the target variables\n",
    "    y_train_scaled = target_scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = target_scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    b_train_scaled = target_scaler_b.fit_transform(b_train.values.reshape(-1, 1)).flatten()\n",
    "    b_test_scaled = target_scaler_b.transform(b_test.values.reshape(-1, 1)).flatten()\n",
    "    s_train_scaled = target_scaler_s.fit_transform(s_train.values.reshape(-1, 1)).flatten()\n",
    "    s_test_scaled = target_scaler_s.transform(s_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, A_train_scaled, A_test_scaled, R_train_scaled, R_test_scaled, \\\n",
    "           y_train_scaled, y_test_scaled, b_train_scaled, b_test_scaled, s_train_scaled, s_test_scaled, \\\n",
    "           target_scaler_y, target_scaler_b, target_scaler_s, feature_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train_scaled, y_train_scaled, A_train_scaled, b_train_scaled, R_train_scaled, s_train_scaled):\n",
    "    model_E = xgb.XGBRegressor(n_estimators=2000, learning_rate=0.5, max_depth=15, subsample=0.9, colsample_bytree=0.8, random_state=42)\n",
    "    model_UTS = xgb.XGBRegressor(n_estimators=2000, learning_rate=0.5, max_depth=15, subsample=0.9, colsample_bytree=0.8, random_state=42)\n",
    "    model_C = xgb.XGBRegressor(n_estimators=2000, learning_rate=0.5, max_depth=15, subsample=0.9, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "    model_E.fit(X_train_scaled, y_train_scaled)\n",
    "    model_UTS.fit(A_train_scaled, b_train_scaled)\n",
    "    model_C.fit(R_train_scaled, s_train_scaled)\n",
    "\n",
    "    return model_E, model_UTS, model_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model_E, model_UTS, model_C, X_test_scaled, A_test_scaled, R_test_scaled,\n",
    "                     target_scaler_y, target_scaler_b, target_scaler_s,\n",
    "                     y_test_scaled, b_test_scaled, s_test_scaled):\n",
    "    yhat_scaled = model_E.predict(X_test_scaled)\n",
    "    inv_yhat = target_scaler_y.inverse_transform(yhat_scaled.reshape(-1, 1)).flatten()\n",
    "    inv_y = target_scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    bhat_scaled = model_UTS.predict(A_test_scaled)\n",
    "    inv_bhat = target_scaler_b.inverse_transform(bhat_scaled.reshape(-1, 1)).flatten()\n",
    "    inv_b = target_scaler_b.inverse_transform(b_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    shat_scaled = model_C.predict(R_test_scaled)\n",
    "    inv_shat = target_scaler_s.inverse_transform(shat_scaled.reshape(-1, 1)).flatten()\n",
    "    inv_s = target_scaler_s.inverse_transform(s_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return inv_y, inv_yhat, inv_b, inv_bhat, inv_s, inv_shat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(inv_y, inv_yhat, inv_b, inv_bhat, inv_s, inv_shat):\n",
    "    rmse1 = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    rmse2 = np.sqrt(mean_squared_error(inv_b, inv_bhat))\n",
    "    rmse3 = np.sqrt(mean_squared_error(inv_s, inv_shat))\n",
    "\n",
    "    print(f'RMSE Elongation: {rmse1:.3f}')\n",
    "    print(f'RMSE UTS: {rmse2:.3f}')\n",
    "    print(f'RMSE Conductivity: {rmse3:.3f}')\n",
    "\n",
    "    pd.DataFrame({'Actual': inv_y, 'Predicted': inv_yhat}).to_csv(results_path / 'test_results_xgboost_Elongation.csv', index=False)\n",
    "    pd.DataFrame({'Actual': inv_b, 'Predicted': inv_bhat}).to_csv(results_path / 'test_results_xgboost_UTS.csv', index=False)\n",
    "    pd.DataFrame({'Actual': inv_s, 'Predicted': inv_shat}).to_csv(results_path / 'test_results_xgboost_Conductivity.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models_and_scalers(model_E, model_UTS, model_C, target_scaler_y, target_scaler_b, target_scaler_s, feature_scaler):\n",
    "    joblib.dump(model_E, model_path / 'xgboost_model_Elongation.joblib')\n",
    "    joblib.dump(model_UTS, model_path / 'xgboost_model_UTS.joblib')\n",
    "    joblib.dump(model_C, model_path / 'xgboost_model_Conductivity.joblib')\n",
    "\n",
    "    joblib.dump(target_scaler_y, model_path / 'target_scaler_y.joblib')\n",
    "    joblib.dump(target_scaler_b, model_path / 'target_scaler_b.joblib')\n",
    "    joblib.dump(target_scaler_s, model_path / 'target_scaler_s.joblib')\n",
    "    joblib.dump(feature_scaler, model_path / 'feature_scaler.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Elongation: 2.400\n",
      "RMSE UTS: 0.512\n",
      "RMSE Conductivity: 0.195\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled, A_train_scaled, A_test_scaled, R_train_scaled, R_test_scaled, \\\n",
    "y_train_scaled, y_test_scaled, b_train_scaled, b_test_scaled, s_train_scaled, s_test_scaled, \\\n",
    "target_scaler_y, target_scaler_b, target_scaler_s, feature_scaler = scale_data(X_train, X_test, A_train, A_test, R_train, R_test,\n",
    "                                                              y_train, y_test, b_train, b_test, s_train, s_test)\n",
    "\n",
    "model_E, model_UTS, model_C = train_models(X_train_scaled, y_train_scaled, A_train_scaled, b_train_scaled, R_train_scaled, s_train_scaled)\n",
    "\n",
    "inv_y, inv_yhat, inv_b, inv_bhat, inv_s, inv_shat = make_predictions(model_E, model_UTS, model_C, X_test_scaled,\n",
    "                                                                     A_test_scaled, R_test_scaled,\n",
    "                                                                     target_scaler_y, target_scaler_b, target_scaler_s,\n",
    "                                                                     y_test_scaled, b_test_scaled, s_test_scaled)\n",
    "\n",
    "evaluate_and_save_results(inv_y, inv_yhat, inv_b, inv_bhat, inv_s, inv_shat)\n",
    "\n",
    "save_models_and_scalers(model_E, model_UTS, model_C, target_scaler_y, target_scaler_b, target_scaler_s, feature_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models and scalers...\n",
      "Models and scalers loaded successfully.\n",
      "Loading data from /home/nitin/Downloads/SIH-1723/data/processed/forward/X_test_Conductivity.csv...\n",
      "Making predictions for the last 10 rows...\n",
      "Predictions completed.\n",
      "   Elongation        UTS  Conductivity\n",
      "0   12.367015  10.606871     61.212036\n",
      "1   11.847616  11.315399     60.921577\n",
      "2   10.155425  11.333567     60.992916\n",
      "3   12.787297  10.483118     61.258614\n",
      "4   13.226456  10.077614     61.660809\n",
      "5    9.848435  11.553331     61.285473\n",
      "6   11.753441  10.764815     61.214531\n",
      "7   12.199645  10.965165     61.338993\n",
      "8   14.248695  10.798165     61.192360\n",
      "9   13.525156  10.115539     61.209919\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_features_columns = [\n",
    "    'EMUL_OIL_L_TEMP_PV_VAL0', \n",
    "    'STAND_OIL_L_TEMP_PV_REAL_VAL0', \n",
    "    'GEAR_OIL_L_TEMP_PV_REAL_VAL0', \n",
    "    'EMUL_OIL_L_PR_VAL0','QUENCH_CW_FLOW_EXIT_VAL0', \n",
    "    'CAST_WHEEL_RPM_VAL0', \n",
    "    'BAR_TEMP_VAL0', \n",
    "    'QUENCH_CW_FLOW_ENTRY_VAL0','GEAR_OIL_L_PR_VAL0', \n",
    "    'STANDS_OIL_L_PR_VAL0', \n",
    "    'TUNDISH_TEMP_VAL0', \n",
    "    'BATH_TEMP_F7_VAL0', \n",
    "    'BATH_TEMP_F8_VAL0', \n",
    "    'RM_MOTOR_COOL_WATER__VAL0', \n",
    "    'ROLL_MILL_AMPS_VAL0', \n",
    "    'RM_COOL_WATER_FLOW_VAL0', \n",
    "    'EMULSION_LEVEL_ANALO_VAL0', \n",
    "    '%AL'\n",
    "]\n",
    "\n",
    "A_features_columns = [\n",
    "    'EMUL_OIL_L_TEMP_PV_VAL0', \n",
    "    'STAND_OIL_L_TEMP_PV_REAL_VAL0', \n",
    "    'GEAR_OIL_L_TEMP_PV_REAL_VAL0', \n",
    "    'EMUL_OIL_L_PR_VAL0','QUENCH_CW_FLOW_EXIT_VAL0', \n",
    "    'CAST_WHEEL_RPM_VAL0', \n",
    "    'BAR_TEMP_VAL0', \n",
    "    'QUENCH_CW_FLOW_ENTRY_VAL0','GEAR_OIL_L_PR_VAL0', \n",
    "    'STANDS_OIL_L_PR_VAL0', \n",
    "    'TUNDISH_TEMP_VAL0', \n",
    "    'BATH_TEMP_F7_VAL0', \n",
    "    'BATH_TEMP_F8_VAL0', \n",
    "    'RM_MOTOR_COOL_WATER__VAL0', \n",
    "    'ROLL_MILL_AMPS_VAL0', \n",
    "    'RM_COOL_WATER_FLOW_VAL0', \n",
    "    'EMULSION_LEVEL_ANALO_VAL0', \n",
    "    '%AL'\n",
    "]\n",
    "\n",
    "R_features_columns = [\n",
    "    'EMUL_OIL_L_TEMP_PV_VAL0', \n",
    "    'STAND_OIL_L_TEMP_PV_REAL_VAL0', \n",
    "    'GEAR_OIL_L_TEMP_PV_REAL_VAL0', \n",
    "    'EMUL_OIL_L_PR_VAL0','QUENCH_CW_FLOW_EXIT_VAL0', \n",
    "    'CAST_WHEEL_RPM_VAL0', \n",
    "    'BAR_TEMP_VAL0', \n",
    "    'QUENCH_CW_FLOW_ENTRY_VAL0','GEAR_OIL_L_PR_VAL0', \n",
    "    'STANDS_OIL_L_PR_VAL0', \n",
    "    'TUNDISH_TEMP_VAL0', \n",
    "    'BATH_TEMP_F7_VAL0', \n",
    "    'BATH_TEMP_F8_VAL0', \n",
    "    'RM_MOTOR_COOL_WATER__VAL0', \n",
    "    'ROLL_MILL_AMPS_VAL0', \n",
    "    'RM_COOL_WATER_FLOW_VAL0', \n",
    "    'EMULSION_LEVEL_ANALO_VAL0', \n",
    "    '%AL'\n",
    "]\n",
    "\n",
    "\n",
    "def load_models_and_predict_last_rows(file_path):\n",
    "    \"\"\"\n",
    "    Load trained models and scalers from disk, load data from an Excel file, and make predictions for the last 10 rows.\n",
    "    \"\"\"\n",
    "    print(\"Loading models and scalers...\")\n",
    "\n",
    "    # Load models\n",
    "    model_E = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/xgboost_model_Elongation.joblib')\n",
    "    model_UTS = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/xgboost_model_UTS.joblib')\n",
    "    model_C = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/xgboost_model_Conductivity.joblib')\n",
    "\n",
    "    # Load scalers\n",
    "    target_scaler_y = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/target_scaler_y.joblib')\n",
    "    target_scaler_b = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/target_scaler_b.joblib')\n",
    "    target_scaler_s = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/target_scaler_s.joblib')\n",
    "\n",
    "    input_scaler_X = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/feature_scaler.joblib')\n",
    "    input_scaler_A = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/feature_scaler.joblib')\n",
    "    input_scaler_R = joblib.load('/home/nitin/Downloads/SIH-1723/ml_models/models/feature_scaler.joblib')\n",
    "\n",
    "    print(\"Models and scalers loaded successfully.\")\n",
    "\n",
    "    # Load data from Excel file\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Select the last 10 rows\n",
    "    last_10_rows = data.tail(10)\n",
    "\n",
    "    # Define feature groups\n",
    "    X_features = last_10_rows[X_features_columns]\n",
    "    A_features = last_10_rows[A_features_columns]\n",
    "    R_features = last_10_rows[R_features_columns]\n",
    "\n",
    "    # Scale the features\n",
    "    X_test_scaled = input_scaler_X.transform(X_features)\n",
    "    A_test_scaled = input_scaler_A.transform(A_features)\n",
    "    R_test_scaled = input_scaler_R.transform(R_features)\n",
    "\n",
    "    # Initialize lists to store predictions\n",
    "    elongation_predictions = []\n",
    "    uts_predictions = []\n",
    "    conductivity_predictions = []\n",
    "\n",
    "    print(\"Making predictions for the last 10 rows...\")\n",
    "    for X_test_row, A_test_row, R_test_row in zip(X_test_scaled, A_test_scaled, R_test_scaled):\n",
    "        # Ensure each row is 2D\n",
    "        X_test_row = X_test_row.reshape(1, -1)\n",
    "        A_test_row = A_test_row.reshape(1, -1)\n",
    "        R_test_row = R_test_row.reshape(1, -1)\n",
    "\n",
    "        # Predictions for Elongation\n",
    "        yhat_scaled = model_E.predict(X_test_row)\n",
    "        inv_yhat = target_scaler_y.inverse_transform(yhat_scaled.reshape(-1, 1)).flatten()\n",
    "        elongation_predictions.append(inv_yhat[0])\n",
    "\n",
    "        # Predictions for UTS\n",
    "        bhat_scaled = model_UTS.predict(A_test_row)\n",
    "        inv_bhat = target_scaler_b.inverse_transform(bhat_scaled.reshape(-1, 1)).flatten()\n",
    "        uts_predictions.append(inv_bhat[0])\n",
    "\n",
    "        # Predictions for Conductivity\n",
    "        shat_scaled = model_C.predict(R_test_row)\n",
    "        inv_shat = target_scaler_s.inverse_transform(shat_scaled.reshape(-1, 1)).flatten()\n",
    "        conductivity_predictions.append(inv_shat[0])\n",
    "\n",
    "    print(\"Predictions completed.\")\n",
    "    \n",
    "    # Combine predictions into a DataFrame\n",
    "    predictions = pd.DataFrame({\n",
    "        'Elongation': elongation_predictions,\n",
    "        'UTS': uts_predictions,\n",
    "        'Conductivity': conductivity_predictions\n",
    "    })\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"/home/nitin/Downloads/SIH-1723/data/processed/forward/X_test_Conductivity.csv\"\n",
    "predictions = load_models_and_predict_last_rows(file_path)\n",
    "\n",
    "# Display the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.93588475, 0.77465302, 0.63375382, ..., 0.31400675, 0.46739094,\n",
       "        0.38848921],\n",
       "       [0.94566101, 0.81056641, 0.69722745, ..., 0.24059132, 0.5467571 ,\n",
       "        0.81294964],\n",
       "       [0.89305318, 0.5851967 , 0.62196362, ..., 0.61282472, 0.35994443,\n",
       "        0.36690647],\n",
       "       ...,\n",
       "       [0.95814703, 0.81503915, 0.658603  , ..., 0.29768046, 0.47942188,\n",
       "        0.49640288],\n",
       "       [0.89620135, 0.65649269, 0.53100159, ..., 0.78870606, 0.46020107,\n",
       "        0.30935252],\n",
       "       [0.90709811, 0.78582553, 0.66011136, ..., 0.31173174, 0.4896359 ,\n",
       "        0.23741007]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
